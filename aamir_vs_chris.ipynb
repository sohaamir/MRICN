{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiIXIgt1dRmocRHlZaFszW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sohaamir/aamir_vs_chris/blob/main/aamir_vs_chris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Who has the bigger brain, me or Chris Gorgolewski? A gentle introduction to visualization using nilearn and nibabel**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oJ9w_yscJn_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Google Colab?\n",
        "**Google Colaboratory** is is a free cloud-based platform provided by Google that offers a Jupyter notebook environment for writing and executing Python code. It is primarily used for data analysis tasks but can also be used for general-purpose Python programming. It has many benefits when working with data (including neuroimaging data) such as:\n",
        "\n",
        "1.   Free access\n",
        "2.   Cloud-based hosting (available everywhere)\n",
        "3.   GPU/TPU Support (lots of processing power)\n",
        "4.   External Data Access (import data GitHub, Google Drive, local machine)\n",
        "\n",
        "It is an interactive environment similar to Anaconda, but with certain advantages (like those mentioned above). Similarly, Colab allows for users to run code in small chunks called 'cells', displaying any output such as images directly within the notebook as well. The programming language used by these notebooks is `Python`, which it organises in the form of `Colab Notebooks`."
      ],
      "metadata": {
        "id": "21WlPOaDTV7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What can we do in Google Colab?\n",
        "\n",
        "We can do a whole bunch of things..."
      ],
      "metadata": {
        "id": "rDOLRjkGR3ch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We can use it for data visualization and plotting"
      ],
      "metadata": {
        "id": "KsOpCKUYXW0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "x = np.linspace(0, 10, 100)\n",
        "y = np.sin(x)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.title(\"Sine Wave\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"sin(X)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sLBCH2zBXeNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def mandelbrot(c, max_iter):\n",
        "    z = 0\n",
        "    n = 0\n",
        "    while abs(z) <= 2 and n < max_iter:\n",
        "        z = z*z + c\n",
        "        n += 1\n",
        "    return n\n",
        "\n",
        "def mandelbrot_image(xmin, xmax, ymin, ymax, width=10, height=10, max_iter=256):\n",
        "    # Create a width x height grid of complex numbers\n",
        "    x = np.linspace(xmin, xmax, width * 100)\n",
        "    y = np.linspace(ymin, ymax, height * 100)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    C = X + 1j * Y\n",
        "\n",
        "    # Compute Mandelbrot set\n",
        "    img = np.zeros(C.shape, dtype=int)\n",
        "    for i in range(width * 100):\n",
        "        for j in range(height * 100):\n",
        "            img[j, i] = mandelbrot(C[j, i], max_iter)\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(width, height))\n",
        "    plt.imshow(img, extent=(xmin, xmax, ymin, ymax), cmap='hot')\n",
        "    plt.colorbar()\n",
        "    plt.title(\"Mandelbrot Set\")\n",
        "    plt.show()\n",
        "\n",
        "# Parameters defining the extent of the region in the complex plane we're going to plot\n",
        "xmin, xmax, ymin, ymax = -2.0, 0.5, -1.25, 1.25\n",
        "width, height = 10, 10\n",
        "max_iter = 256\n",
        "\n",
        "mandelbrot_image(xmin, xmax, ymin, ymax, width, height, max_iter)"
      ],
      "metadata": {
        "id": "6txgqlC7bUs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Watch YouTube"
      ],
      "metadata": {
        "id": "Vo2qyNzbXtk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "\n",
        "# Embedding the YouTube video\n",
        "YouTubeVideo('GtL1huin9EE', width=800, height=450)"
      ],
      "metadata": {
        "id": "5BoCUY2bXxo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning about neuroimaging"
      ],
      "metadata": {
        "id": "X6gFinutYyMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Cute Cat](https://nipy.org/nibabel/_static/nibabel-logo.svg)\n"
      ],
      "metadata": {
        "id": "3nfd4A0iP5Wm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are nibabel and nilearn?\n",
        "\n",
        "## nibabel\n",
        "\n",
        "`nibabel` is designed to provide a simple, uniform interface to various neuroimaging file formats, allowing users to easily manipulate neuroimaging data within Python scripts or interactive environments like Jupyter notebooks.\n",
        "\n",
        "Key features and capabilities of `nibabel` include:\n",
        "\n",
        "- Reading and Writing Neuroimaging Data: `nibabel` allows you to read data from disk into Python data structures and write data back to disk in various neuroimaging formats.\n",
        "\n",
        "- Data Manipulation: Once loaded into Python, neuroimaging data can be manipulated just like any other data structure. This includes operations like slicing, statistical analyses, and visualization.\n",
        "\n",
        "- Header Access: `nibabel` provides access to the headers of neuroimaging files, which contain metadata about the imaging data such as dimensions, voxel sizes, data type, and orientation. This is crucial for understanding and correctly interpreting the data.\n",
        "\n",
        "- Affine Transformations: It supports affine transformations that describe the relationship between voxel coordinates and world coordinates, enabling spatial operations on the data.\n",
        "\n",
        "## nilearn\n",
        "\n",
        "`nilearn` is a Python library designed to facilitate fast and easy statistical learning analysis and manipulation of neuroimaging data. It builds on libraries such as numpy, scipy, scikit-learn, and pandas, offering a comprehensive toolkit for neuroimaging data processing, with a focus on machine learning applications. `nilearn` aims to make it easier for researchers in neuroscience and machine learning to use Python for sophisticated imaging data analysis and visualization.\n",
        "\n",
        "In this intro, we won't be focusing on the more advanced uses such as those involving machine learning, but just leveraging it's ability to perform simple functions with structural MRI images.\n",
        "\n",
        "To this end, one of the strengths of `nilearn` is its powerful and flexible plotting capabilities, designed specifically for neuroimaging data. It provides functions to visualize MRI volumes, statistical maps, connectome diagrams, and more, with minimal code."
      ],
      "metadata": {
        "id": "YGOKCc7hRz-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we get started, we need to install the necessary packages and import the NIFTIs. We will use T1-weighted structural MRI scans, one of myself and one that I copied from [OpenNeuro](https://openneuro.org/), a website for openly available neuroimaging datasets."
      ],
      "metadata": {
        "id": "yRuZZCpxTAmz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAI5gApbGpG0"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install nibabel matplotlib\n",
        "!pip install imageio\n",
        "\n",
        "# Download the MRI scan files from GitHub and rename them\n",
        "!wget https://github.com/sohaamir/aamir_vs_chris/raw/main/sub-CG_T1w.nii -O chris_brain.nii\n",
        "!wget https://github.com/sohaamir/aamir_vs_chris/raw/main/sub-0002_ses-1_acq-mprage_rec-NORM_T1w.nii -O aamir_brain.nii\n",
        "\n",
        "import nibabel as nib\n",
        "from nilearn import plotting\n",
        "import numpy as np\n",
        "import pylab as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What can we do with our data using nibabel?"
      ],
      "metadata": {
        "id": "FOrs6lihSZm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below loads a NIfTI image and prints out its shape and data type. The shape tells us the dimensions of the image, which typically includes the number of voxels in each spatial dimension (X, Y, Z) and sometimes time or other dimensions. The data type indicates the type of data used to store voxel values, such as float or integer types.\n"
      ],
      "metadata": {
        "id": "fqlWRWXEUbeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the first image using nibabel\n",
        "t1_aamir_path = 'aamir_brain.nii'\n",
        "t1_aamir_image = nib.load(t1_aamir_path)\n",
        "\n",
        "# Now let's check the image data shape and type\n",
        "print(\"Aamir's image shape:\", t1_aamir_image.shape)\n",
        "print(\"Aamir's image data type:\", t1_aamir_image.get_data_dtype())"
      ],
      "metadata": {
        "id": "eLNOMNzASnBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each NIfTI file also comes with a header containing metadata about the image. Here we've extracted the voxel sizes, which represent the physical space covered by each voxel, and the image orientation, which tells us how the image data is oriented in space."
      ],
      "metadata": {
        "id": "WppNRMp1Unwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the image header\n",
        "header_aamir = t1_aamir_image.header\n",
        "\n",
        "# Print some header information\n",
        "print(\"Aamir's voxel size:\", header_aamir.get_zooms())\n",
        "print(\"Aamir's image orientation:\", nib.aff2axcodes(t1_aamir_image.affine))"
      ],
      "metadata": {
        "id": "sDdk0o4OUu6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize the data from our NIfTI file by converting it to a numpy array and then using `matplotlib` to display a slice. In this example, we've displayed an axial slice from the middle of the brain, which is a common view for inspecting T1-weighted images."
      ],
      "metadata": {
        "id": "g3hnyxLEU0k8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the image data as a numpy array\n",
        "aamir_data = t1_aamir_image.get_fdata()\n",
        "\n",
        "# Display one axial slice of the image\n",
        "aamir_slice_index = aamir_data.shape[2] // 2  # Get the middle index along Z-axis\n",
        "plt.imshow(aamir_data[:, :, aamir_slice_index], cmap='gray')\n",
        "plt.title('Middle Axial Slice of Aamir Brain')\n",
        "plt.axis('off')  # Hide the axis to better see the image\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OTeF09qwU09v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can load a second T1-weighted image and printed its shape for comparison. By comparing the shapes of the two images, we can determine if they are from the same scanning protocol or if they need to be co-registered for further analysis."
      ],
      "metadata": {
        "id": "fbAQW2p4XO4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the second image\n",
        "t1_image_2_path = 'sub-0002_ses-1_acq-mprage_rec-NORM_T1w.nii'\n",
        "t1_image_2 = nib.load(t1_image_2_path)\n",
        "\n",
        "# Let's compare the shapes of the two images\n",
        "print(\"First image shape:\", t1_image.shape)\n",
        "print(\"Second image shape:\", t1_image_2.shape)"
      ],
      "metadata": {
        "id": "qe4t42hUXP1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have loaded both T1-weighted MRI images, we are interested in visualizing and comparing them directly. To do this, we will extract the data from the second image, `sub-0002_ses-1_acq-mprage_rec-NORM_T1w.nii`, and display a slice from it alongside a slice from the first image, `sub-CG_T1w.nii`.\n"
      ],
      "metadata": {
        "id": "0f4i9wl9XZcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data for the second image\n",
        "image_data_2 = t1_image_2.get_fdata()\n",
        "\n",
        "# Display a slice from both images side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Plot first image slice\n",
        "axes[0].imshow(image_data[:, :, slice_index], cmap='gray')\n",
        "axes[0].set_title('aamir_brain.nii')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Plot second image slice\n",
        "axes[1].imshow(image_data_2[:, :, slice_index], cmap='gray')\n",
        "axes[1].set_title('chris_brain.nii')\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AupisA44XZ8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Animated GIFs from MRI Scans\n",
        "\n",
        "To visualize the structure of the brain in MRI scans, we can create an animated GIF that scrolls through each slice of the scan. This is particularly useful for examining the scan in a pseudo-3D view by observing one slice at a time through the entire depth of the brain.\n",
        "\n",
        "The following code defines a function `create_gif_from_mri_normalized` that processes an MRI scan file and produces an animated GIF. The MRI data is first normalized by clipping the top and bottom 1% of pixel intensities, which enhances contrast and detail. The scan is then sliced along the sagittal plane, and each slice is converted to an 8-bit grayscale image and compiled into an animated GIF. This normalization process ensures that the resulting GIF maintains visual consistency across different scans.\n",
        "\n",
        "We apply this function to two MRI scans, `aamir_brain.nii` and `chris_brain.nii`, creating a GIF for each. These GIFs, named 'aamir_brain_normalized.gif' and 'chris_brain_normalized.gif', respectively, will allow us to visually assess and compare the scans."
      ],
      "metadata": {
        "id": "nL5WUyY9aUt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to normalize and create a GIF from a 3D MRI scan in the sagittal plane\n",
        "def create_gif_from_mri_normalized(path, gif_name):\n",
        "    # Load the image and get the data\n",
        "    img = nib.load(path)\n",
        "    data = img.get_fdata()\n",
        "\n",
        "    # Normalize the data for better visualization\n",
        "    # Clip the top and bottom 1% of pixel intensities\n",
        "    p2, p98 = np.percentile(data, (2, 98))\n",
        "    data = np.clip(data, p2, p98)\n",
        "    data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "\n",
        "    # Prepare to capture the slices\n",
        "    slices = []\n",
        "    # Sagittal slices are along the x-axis, hence data[x, :, :]\n",
        "    for i in range(data.shape[0]):\n",
        "        slice = data[i, :, :]\n",
        "        slice = np.rot90(slice)  # Rotate or flip the slice if necessary\n",
        "        slices.append((slice * 255).astype(np.uint8))  # Convert to uint8 for GIF\n",
        "\n",
        "    # Create a GIF\n",
        "    imageio.mimsave(gif_name, slices, duration=0.1)  # duration controls the speed of the GIF\n",
        "\n",
        "# Create GIFs from the MRI scans\n",
        "create_gif_from_mri_normalized('aamir_brain.nii', 'aamir_brain_normalized.gif')\n",
        "create_gif_from_mri_normalized('chris_brain.nii', 'chris_brain_normalized.gif')\n"
      ],
      "metadata": {
        "id": "8AQtkwD1aVkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Displaying MRI Scan GIFs\n",
        "\n",
        "After generating the GIFs for each MRI scan, we can now display them directly within the notebook. This visualization provides us with an interactive view of the scans, making it easier to observe the entire brain volume as a continuous animation.\n",
        "\n",
        "Below, we use the `IPython.display` module to render the GIFs in the notebook. The first GIF corresponds to Aamir's brain scan, and the second GIF corresponds to Chris's brain scan. These inline animations can be a powerful tool for presentations, education, and qualitative analysis, offering a dynamic view into the MRI data without the need for specialized neuroimaging software."
      ],
      "metadata": {
        "id": "4t59_MeGaegJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Display the GIF for Aamir's MRI\n",
        "display(Image(filename='aamir_brain_normalized.gif'))\n",
        "\n",
        "# Display the GIF for Chris's MRI\n",
        "display(Image(filename='chris_brain_normalized.gif'))"
      ],
      "metadata": {
        "id": "olrJMruian31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What can we do with our data using nilearn?"
      ],
      "metadata": {
        "id": "nAJ6cZQrYpWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Brain Overlays with Nilearn\n",
        "\n",
        "With `nilearn`, we can create informative visualizations of brain images. One common technique is to overlay a statistical map or a labeled atlas on top of an anatomical image for better spatial context. Here, we will demonstrate how to overlay a standard atlas on our T1-weighted MRI images. This allows us to see how different brain regions delineated by the atlas correspond to structures within the actual brain images."
      ],
      "metadata": {
        "id": "r9NQehG7Y3xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nilearn import plotting, datasets\n",
        "\n",
        "# Load the atlas\n",
        "atlas_data = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\n",
        "\n",
        "# Plotting the atlas map overlay on the first image\n",
        "plotting.plot_roi(atlas_data.maps, bg_img='aamir_brain.nii', title=\"Aamir's Brain with Atlas Overlay\", display_mode='ortho', cut_coords=(0, 0, 0), cmap='Paired')\n",
        "\n",
        "# Plotting the atlas map overlay on the second image\n",
        "plotting.plot_roi(atlas_data.maps, bg_img='chris_brain.nii', title=\"Chris's Brain with Atlas Overlay\", display_mode='ortho', cut_coords=(0, 0, 0), cmap='Paired')\n",
        "\n",
        "plotting.show()"
      ],
      "metadata": {
        "id": "-O5DevF8Ytbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting Connectome Features with Nilearn\n",
        "\n",
        "`nilearn` can also be used to compute connectivity matrices, which represent the connections in the brain, typically from fMRI data. While our data is from T1-weighted MRIs and not functional scans, we can still illustrate the process. If we had resting-state fMRI (rs-fMRI) data for these subjects, we could compute the functional connectome, which is a powerful tool for understanding brain network organization."
      ],
      "metadata": {
        "id": "z8LvrsyOY9dC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nilearn.connectome import ConnectivityMeasure\n",
        "\n",
        "# For demonstration purposes, let's assume we have preprocessed fMRI time series data for regions of interest (ROIs)\n",
        "# Here we'll just create some random time series data\n",
        "import numpy as np\n",
        "time_series_aamir = np.random.rand(200, 5)  # 200 time points, 5 ROIs\n",
        "time_series_chris = np.random.rand(200, 5)\n",
        "\n",
        "# Compute correlation matrices\n",
        "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
        "correlation_matrix_aamir = correlation_measure.fit_transform([time_series_aamir])[0]\n",
        "correlation_matrix_chris = correlation_measure.fit_transform([time_series_chris])[0]\n",
        "\n",
        "# Now let's plot the correlation matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot for Aamir's brain\n",
        "axes[0].matshow(correlation_matrix_aamir, cmap='coolwarm')\n",
        "axes[0].set_title(\"Aamir's Functional Connectome\")\n",
        "\n",
        "# Plot for Chris's brain\n",
        "axes[1].matshow(correlation_matrix_chris, cmap='coolwarm')\n",
        "axes[1].set_title(\"Chris's Functional Connectome\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oKcUJe9MY_qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performing Statistical Analysis on Brain Images\n",
        "\n",
        "`nilearn` provides tools to perform statistical tests on brain images. For example, we could use it to compare voxel-wise differences between two groups of images. This type of analysis is common in studies looking for brain changes associated with a condition or disease. Below, we demonstrate a simple voxel-wise t-test between our two brain images as an illustrative example."
      ],
      "metadata": {
        "id": "NsIQIeazZxXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nilearn.image import math_img\n",
        "\n",
        "# Load the brain images into nilearn\n",
        "img_aamir = nib.load('aamir_brain.nii')\n",
        "img_chris = nib.load('chris_brain.nii')\n",
        "\n",
        "# Perform a voxel-wise t-test between the images\n",
        "# Note: This is a simplistic example for illustration purposes. Proper statistical tests would require a sample of images.\n",
        "t_test_result = math_img('img1 - img2', img1=img_aamir, img2=img_chris)\n",
        "\n",
        "# Visualize the t-test results\n",
        "plotting.plot_stat_map(t_test_result, title=\"T-test Result (Aamir - Chris)\", threshold=3)\n",
        "\n",
        "plotting.show()"
      ],
      "metadata": {
        "id": "nl0SfgruZyOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seeing who has the bigger brain"
      ],
      "metadata": {
        "id": "3kVVyHBuatkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the size of the brains\n",
        "# Here we'll just count the non-zero voxels as a proxy for brain size\n",
        "# This is a naive approach; in practice you would want a more sophisticated method\n",
        "size_img1 = np.count_nonzero(img1.get_fdata())\n",
        "size_img2 = np.count_nonzero(img2.get_fdata())\n",
        "\n",
        "print(f\"Size of brain in the first image: {size_img1} voxels\")\n",
        "print(f\"Size of brain in the second image: {size_img2} voxels\")\n",
        "\n",
        "# Compare sizes\n",
        "if size_img1 > size_img2:\n",
        "    print(\"The first brain is larger.\")\n",
        "elif size_img1 < size_img2:\n",
        "    print(\"The second brain is larger.\")\n",
        "else:\n",
        "    print(\"The brains are the same size.\")"
      ],
      "metadata": {
        "id": "JBiPzlAOKFXC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}